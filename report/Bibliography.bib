@article{RN1,
   author = {Gorti, Mathieu Ravaut and Satya Krishna},
   title = {Faster gradient descent via an adaptive learning rate},
   url = {http://www.cs.toronto.edu/~mravox/p4.pdf},
   year = {2017},
   type = {Journal Article}
}

@article{RN2,
   author = {Gu, Haibing Wu and  Xiaodong},
   title = {Max-Pooling Dropout for Regularization of
Convolutional Neural Networks},
   url = {https://arxiv.org/pdf/1512.01400.pdf},
   type = {Journal Article}
}

@article{RN3,
   author = {Sabuncu, Zhilu Zhang and Mert R.},
   title = {Generalized Cross Entropy Loss for Training Deep
Neural Networks with Noisy Labels},
   url = {https://papers.nips.cc/paper/8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.pdf},
   type = {Journal Article}
}

@article{RN4,
   author = {Szegedy, Sergey Ioffe and Christian},
   title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
   url = {https://arxiv.org/pdf/1502.03167.pdf},
   year = {2015},
   type = {Journal Article}
}

@article{RN5,
   author = {Visin, Vincent Dumoulin and Francesco},
   title = {A guide to convolution arithmetic for deep learning},
   url = {https://arxiv.org/pdf/1603.07285.pdf},
   year = {2018},
   type = {Journal Article}
}

@article{RN6,
   author = {Dr.A.Santhakumaran, T.Jayalakshmi and},
   title = {Statistical Normalization and Back Propagation
for Classification},
   url = {https://pdfs.semanticscholar.org/4cbd/5fcd6081cfcd16e9111f1bcc17b6e283d439.pdf},
   year = {2011},
   type = {Journal Article}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@Inbook{LeCun1998,
author="LeCun, Yann
and Bottou, Leon
and Orr, Genevieve B.
and M{\"u}ller, Klaus -Robert",
editor="Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--50",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
isbn="978-3-540-49430-0",
doi="10.1007/3-540-49430-8_2",
url="https://doi.org/10.1007/3-540-49430-8_2"
}

@misc{RN13,
   author = {Benchmarks.Ai},
   title = {Classify 32x32 colour images into 10 categories},
   url = {https://benchmarks.ai/cifar-10},
   year = {2019},
   type = {Web Page}
}

@misc{RN14,
   author = {Code, Papers with},
   title = {Image Classification on CIFAR-10},
   url = {https://paperswithcode.com/sota/image-classification-on-cifar-10},
   year = {2019},
   type = {Web Page}
}