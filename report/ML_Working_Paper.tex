\documentclass[journal]{IEEEtran}
\renewcommand\thesection{\arabic{section}} 
\renewcommand\thesubsectiondis{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\alph{subsubsection}}
\renewcommand\theparagraphdis{\arabic{paragraph}.}
\usepackage{xcolor,soul,framed} %,caption
\colorlet{shadecolor}{yellow}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsfonts}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}
%=== TITLE & AUTHORS ====================================================================
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
    \title{\includegraphics[width=2.5in]{photo/0_cityu} \\
    Image  classification on the cifar10 dataset using Tensorflow"\\
     \textit{Working Paper for CS4487 Machine Learning}
     }
  \author{BOHNSTEDT 	Timo ,
      L\"OHR Tim\\ 
}
\markboth{Project Report
Machine Learning for Business IS4861 
}{Roberg \MakeLowercase{\textit{et al.}}}

% ====================================================================
\maketitle
% === ABSTRACT 
\begin{abstract}
%\boldmath
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam ac enim vulputate ipsum pellentesque bibendum imperdiet id nibh. Mauris non varius odio. Pellentesque eu libero porta, porttitor lectus ut, dictum neque. Curabitur maximus, justo non faucibus tristique, tellus turpis ornare elit, et sagittis dui justo convallis ex. Pellentesque a libero dui. In vel lobortis nunc. dui. Vivamus congue nulla.
\end{abstract}

% === KEYWORDS 
\begin{IEEEkeywords}
\hl{cifar10, machine learning, data science, image classification}
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

% === I. Problem description ============================
\section{Problem Description}
\IEEEPARstart{T}{}In our course Machine Learning at the City University of Hong Kong, Dr Kede teched us the fundamental mathematical knowledge to solve machine learning tasks. Furthermore, we improved our ability to use this knowledge while working on Jupyter Notebook Tutorials, which were provided by Dr Kede and his assistant PhD students. To proof our learning progress in the theoretical and practical field, we are going to work on a Group Project. To solve an image classification task, we use the widely used dataset "cifar10". The original dataset consists of 60000 coloured images of objects from 10 classes, with 6000 images per category. There are 50, 000 training images and 10, 000 test images. To compare our work with other groups, we are using the following evaluation criteria:
\begin{equation}
Acc(f,D) = \frac{1}{m}\sum_{m}^{i=1} \mathbb{I} \left [ y^{(i)} =f(x^{()i)})\right ]
\end{equation}

\noindent We can choose the Tensorflow Framework from Google and the Facebook pendant Pytorch. To have a quick start, we got two tutorials which are focusing on bothe frameworks and how to use them while solving an image classification task. Because of the broader Community and after a first evaluation based on the provided evaluation criteria, the group decided to use Tensorflow instead of Pytorch.
% === II. Literature Survey ============================
\section{Literature Survey}
\subsection{State of the Art analysis}
\subsection{Open Research Fields}
%
% === III. Technical Details ============================
%
\section{Technical Details}
\subsection{Preprocessing}
\subsubsection{Data Augmentation}
Data augmentation describes the method of creating new data points by transforming the original data. If we look at images, this can be done for example by resizing, rotating, shifting, flipping and more.
With this process we can artificially increase the amount of data, eventhough we have limited access to only 50000 training data. 
This leads to a improved performance for our VGG model. \\
We use the \textit{ImageDataGenerator class} built-in function from Keras itself to perform this transformations.

\subsubsection{Normalization}
The goal of normalization is to make every datapoint have the same scale so each feature is equally important. There are many possible ways to scale data e.g. MinMax-Scaling or Z-Score and all of them follow certain purposes. 
The MinMax-Scaling suffers from the so called \textit{outlier issue}.
We decided to choose the Z-Scoring, because the Z-score normalization is a strategy of normalizing data that avoids this outlier issue \cite{RN6}.  

\subsubsection{Categorical Output}
As the name of CIFAR10 already implies, does this dataset contain of ten different classes. There are ten different animals given by the creators of this dataset. To predict one of those animals with the trained model, the model outputs ten different values, each represents the probability of being one of the animals respectively. 
We convert the aranged numbers from one to ten 50000, 1) into a binary output class matrix (50000, 10). Each row in this matrix values one for the desired animal. E.g. animal number three will be transformed into [0 0 1 0 0 0 0 0 0 0].
If we didn't convert our labels into this format, we could not resolve back the probability for each animal and would lose information to improve. 

\subsection{Model}
\subsubsection{Convolutional Layers}
\subsubsection{Kernel Regularization}
There are many different regularizations to prevent the neural network from overfitting. For our project we chose the \textit{L2 regularization}, because it is the most common form.
It can be integrated by penalizing the squared magnitude of all parameters directly in the objective. So, for every weight \textit{w} in the network, the term \(\frac{1}{2} \lambda w^2\) gets added to the objective, where \(\lambda \) is the regularization strength. It's usual to see \(\frac{1}{2}\) in the front, because then the gradient of this term with respect to \textit{w} is simply \(\lambda w\) instead of \(2 \lambda w\). The L2 penalizes the peaky weight vectors and prefers the diffuse weight vectors \cite{RN4}. During the gradien descent weight update, the L2 regularization means, that every weight is decayed linearly (\textit{w += -lambda * w}) towards zero. \\
\subsubsection{Batch Normalization}
A recently developed technique by Ioffe and Szegedy \cite{RN4} called Batch Normalization  properly initializes neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. Normalization is a simple differentiable operation. 
It allowes to use higher learning rates at the beginning and is less vulnerable to a bad initialization. In other words, neural networks that implement batch normalization layers are significantly more robust. Additionally, batch normalization can be interpreted as a preprocessing step on every layer of the network. Batch normalization yields in general a substantial improvement in the training process. \\
\subsubsection{Activation Functions}
There a couple of widely used activation functions like tanh, sigmoid function, ReLU or the ELU. For our model we decided to use the \textit{ReLU} activation function. The Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function \(f(x)=max(0,x)\). In other words, the activation is simply thresholded at zero. There are plenty of pro's and con's for the ReLU:
\begin{itemize}
\item (+) Compared to tanh/sigmoid neurons that need to compute expensive operations  like the exponentials, the ReLU can be implemented by directly thresholding a matrix of activations at zero.
\item (+) It was found to notably accelerate the convergence of stochastic gradient descent (SGD) compared to the tanh/sigmoid functions. That is, because of its linear, non-saturating form.
\item (-) Sadly, ReLU units can be weak during training and possibly “die”. For example, a large gradient streaming through a ReLU neuron could cause the weights to update in a dead end, so that it will never activate again. If this happens, then the gradient streaming through the unit will forever be zero. The ReLU units can irreversibly die during training since they can get eliminated off the data manifold. With a good scheduling setting of the learning rate this is less likely to happen. \\
\end{itemize}
\subsubsection{Max Pooling}
It is common to periodically add a pooling layer in-between the convolutional layers. Its function is to avoid overfitting and step by step reduce the size of the input image by reducing the amount of parameters and computations of the network. We use three pooling layers with filters of size 3x3 applied with a stride of 0 and a padding set to \textit{same}. This downsamples every depth slice in the input by 2 along both width and height, discarding 50\% of the activations. \\
\subsubsection{Dense Layer}
The Dense layer is the last layer of our model. It is also called the \textit{fully connected layer}. Neurons in the fully connected layer have full connections to all activations in the previous layer, as in the normal neural networks. Their activation functions will then be computed by a matrix multiplication which is followed by a bias offset. 
For the ten different animals we repectively have ten fully connected neurons in the dense layer. \\
\subsubsection{Flatten}
In between the convolutional layer (CNN) and the fully connected layer (Dense), there is a \textit{Flatten layer}. The Flattening layer transforms a two-dimensional matrix of features into a one-dimensional vector that can be respectively streamed into the fully connected neural network classifier, which are our ten fully connected animal neurons. \\
%
% === IV. Result Analysis============================
%
\section{Result Analysis}
\subsection{Evaluation Criteria}
\subsubsection{Overfitting and Generalisation}
\subsubsection{Possible Improvements}
% ============================================
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Bibliography}
\vfill
\end{document}