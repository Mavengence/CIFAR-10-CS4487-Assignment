\documentclass[journal]{IEEEtran}
\renewcommand\thesection{\arabic{section}} 
\renewcommand\thesubsectiondis{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\alph{subsubsection}}
\renewcommand\theparagraphdis{\arabic{paragraph}.}
\usepackage{xcolor,soul,framed} %,caption
\colorlet{shadecolor}{yellow}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[cmex10]{amsmath}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}
\usepackage{amsfonts}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{graphicx}
%
%=== TITLE & AUTHORS ====================================================================
%
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
    \title{\includegraphics[width=2.5in]{photo/0_cityu} \\
    Image  Classification on the CIFAR10 Dataset using Tensorflow"\\
     \textit{Working Paper for CS4487 Machine Learning}
     }
  \author{BOHNSTEDT 	Timo ,
      L\"OHR Tim\\ 
}
\markboth{Project Report
Machine Learning for Business IS4861 
}{Roberg \MakeLowercase{\textit{et al.}}}

% ====================================================================
\maketitle
% === ABSTRACT 
\begin{abstract}
%\boldmath
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam ac enim vulputate ipsum pellentesque bibendum imperdiet id nibh. Mauris non varius odio. Pellentesque eu libero porta, porttitor lectus ut, dictum neque. Curabitur maximus, justo non faucibus tristique, tellus turpis ornare elit, et sagittis dui justo convallis ex. Pellentesque a libero dui. In vel lobortis nunc. dui. Vivamus congue nulla.
\end{abstract}

% === KEYWORDS 
\begin{IEEEkeywords}
\hl{cifar10, machine learning, data science, image classification}
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

% === I. Problem description ============================
\section{Problem Description}
\IEEEPARstart{T}{}In our course Machine Learning at the City University of Hong Kong, Dr Kede teched us the fundamental mathematical knowledge to solve machine learning tasks. Furthermore, we improved our ability to use this knowledge while working on Jupyter Notebook Tutorials, which were provided by Dr Kede and his assistant PhD students. To proof our learning progress in the theoretical and practical field, we are going to work on a Group Project. To solve an image classification task, we use the widely used dataset "cifar10". The original dataset consists of 60000 coloured images of objects from 10 classes, with 6000 images per category. There are 50, 000 training images and 10, 000 test images. To compare our work with other groups, we are using the following evaluation criteria:
\begin{equation}
Acc(f,D) = \frac{1}{m}\sum_{m}^{i=1} \mathbb{I} \left [ y^{(i)} =f(x^{()i)})\right ]
\label{acc}
\end{equation}

\noindent We can choose the Tensorflow Framework from Google and the Facebook pendant Pytorch. To have a quick start, we got two tutorials which are focusing on bothe frameworks and how to use them while solving an image classification task. Because of the broader Community and after a first evaluation based on the provided evaluation criteria, the group decided to use Tensorflow instead of Pytorch. \\
\begin{figure}
  \begin{center}
  \includegraphics[width=3in]{photo/image32.png}\\
  \caption{Cifar 10 pictures}\label{cifar10}
  \end{center}
\end{figure}
% === II. Literature Survey ============================
\section{Literature Survey}
\subsection{State of the Art analysis}
Since the Cifar10 dataset got released in the year 2009, a lot of people have tried to further improve the accuracy on this dataset. There are every year new ground breaking solutions, which further beat the best models. At the current time \textit{2019-12-12}, this is the state-of-the-art technologie from two independent sources \cite{RN13} \cite{RN14}. \\
\begin{figure}
  \begin{center}
  \includegraphics[width=4.3in]{photo/related_work.png}\\
  \caption{State of the Art}\label{state of the art}
  \end{center}
\end{figure} \\
The illustration shows that the current state-of-the-art model achieves 99\%. 
\subsection{Open Research Fields}



%
% === III. Technical Details ============================
%
\section{Technical Details}
\subsection{Preprocessing}
\subsubsection{Data Augmentation}
Data augmentation describes the method of creating new data points by transforming the original data. If we look at images, this can be done for example by resizing, rotating, shifting, flipping and more.
With this process we can artificially increase the amount of data, eventhough we have limited access to only 50000 training data. 
This leads to a improved performance for our VGG model. \\
We use the \textit{ImageDataGenerator class} built-in function from Keras itself to perform this transformations. \\

\subsubsection{Normalization}
The goal of normalization is to make every datapoint have the same scale so each feature is equally important. There are many possible ways to scale data e.g. MinMax-Scaling or Z-Score and all of them follow certain purposes. 
The MinMax-Scaling suffers from the so called \textit{outlier issue}.
We decided to choose the Z-Scoring, because the Z-score normalization is a strategy of normalizing data that avoids this outlier issue \cite{RN6}. \\

\subsubsection{Categorical Output}
As the name of CIFAR10 already implies, does this dataset contain of ten different classes. There are ten different animals given by the creators of this dataset. To predict one of those animals with the trained model, the model outputs ten different values, each represents the probability of being one of the animals respectively. 
We convert the aranged numbers from one to ten 50000, 1) into a binary output class matrix (50000, 10). Each row in this matrix values one for the desired animal. E.g. animal number three will be transformed into [0 0 1 0 0 0 0 0 0 0].
If we didn't convert our labels into this format, we could not resolve back the probability for each animal and would lose information to improve. \\

\subsection{Model}
To solve the image classification task, we are using a convolutional neuronal network (CNN). CNN's are a particular case of feed-forward neural networks \cite{Goodfellow-et-al-2016}. On a very fundamental level, we can say that a feed-forward neural network -as the most machine learning models - is a function. The function of a feed-forward neuronal network can simply descriped as \(y = f(x, \o )\) . CNN's and feed-forward neuronal networks are estimating parameter values. So for \(y = f(x, \o )\) we estimate the parameter \(\o \) \cite{Goodfellow-et-al-2016}. Through those estimated parameter values, we are getting a function with the smallest possible difference between the predicted output values and the defined output values. A function that measures the difference between the expected - and the defined output is a so-called loss function \cite{Goodfellow-et-al-2016}. The problem description gives already a mathematical definition \eqref{acc} from the accuracy loss function which we are using. 
CNN's and classical feed-forward neuronal networks differ in their basis of calculation. Feed-forward neuronal networks are using matrix multiplication, whereas CNN's are using convolutions. Convolutional layers, pooling layers, and fully-connected layers are specific for  a CNN \cite{LeCun1998}. \\
\subsubsection{Convolutional Layers}
ToDo Timo \\
\subsubsection{Pooling Layers}
It is common to periodically add a pooling layer in-between the convolutional layers. Its function is to avoid overfitting and step by step reduce the size of the input image by reducing the amount of parameters and computations of the network. We use three pooling layers with filters of size 3x3 applied with a stride of 0 and a padding set to \textit{same}. This downsamples every depth slice in the input by 2 along both width and height, discarding 50\% of the activations. \\
\subsubsection{Fully-Connected Layers}
The Dense layer is the last layer of our model. It is also called the \textit{fully connected layer}. Neurons in the fully connected layer have full connections to all activations in the previous layer, as in the normal neural networks. Their activation functions will then be computed by a matrix multiplication which is followed by a bias offset. 
For the ten different animals we repectively have ten fully connected neurons in the dense layer. \\
\subsubsection{Kernel Regularization}
There are many different regularizations to prevent the neural network from overfitting. For our project we chose the \textit{L2 regularization}, because it is the most common form.
It can be integrated by penalizing the squared magnitude of all parameters directly in the objective. So, for every weight \textit{w} in the network, the term \(\frac{1}{2} \lambda w^2\) gets added to the objective, where \(\lambda \) is the regularization strength. It's usual to see \(\frac{1}{2}\) in the front, because then the gradient of this term with respect to \textit{w} is simply \(\lambda w\) instead of \(2 \lambda w\). The L2 penalizes the peaky weight vectors and prefers the diffuse weight vectors \cite{RN4}. During the gradien descent weight update, the L2 regularization means, that every weight is decayed linearly (\textit{w += -lambda * w}) towards zero. \\
\subsubsection{Batch Normalization}
A recently developed technique by Ioffe and Szegedy \cite{RN4} called Batch Normalization  properly initializes neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training. Normalization is a simple differentiable operation. 
It allowes to use higher learning rates at the beginning and is less vulnerable to a bad initialization. In other words, neural networks that implement batch normalization layers are significantly more robust. Additionally, batch normalization can be interpreted as a preprocessing step on every layer of the network. Batch normalization yields in general a substantial improvement in the training process. \\
\subsubsection{Activation Functions}
There a couple of widely used activation functions like tanh, sigmoid function, ReLU or the ELU. For our model we decided to use the \textit{ReLU} activation function. The Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function \(f(x)=max(0,x)\). In other words, the activation is simply thresholded at zero. There are plenty of pro's and con's for the ReLU:
\begin{itemize}
\item (+) Compared to tanh/sigmoid neurons that need to compute expensive operations  like the exponentials, the ReLU can be implemented by directly thresholding a matrix of activations at zero.
\item (+) It was found to notably accelerate the convergence of stochastic gradient descent (SGD) compared to the tanh/sigmoid functions. That is, because of its linear, non-saturating form.
\item (-) Sadly, ReLU units can be weak during training and possibly “die”. For example, a large gradient streaming through a ReLU neuron could cause the weights to update in a dead end, so that it will never activate again. If this happens, then the gradient streaming through the unit will forever be zero. The ReLU units can irreversibly die during training since they can get eliminated off the data manifold. With a good scheduling setting of the learning rate this is less likely to happen. \\
\end{itemize}
\subsubsection{Loss function}
Our compiled Keras model uses the \textit{cross-entropy-loss}. 
\begin{equation}
L_{i} = f_{y_{i}} + log \sum_{j} e^{f_{j}}
\end{equation}
where we are using the notation \(f_{j}\) to mean the \textit{j-th} element of the vector of class scores \textit{f}. The full loss for the dataset is the mean of \(f_{i}\) over all training examples together with a regularization term \textit{R(W)}.
The cross-entropy loss, or also called log loss, measures the performance of our classification model with the output as probability values between zero and one. The cross-entropy loss increases as the predicted probability diverges from real value labels.
\subsubsection{Optimizer}
There are many possible optimizer suitable for our task. The common ones are the RMSprob, Adam or the stochastic gradient descent (SGD). We decided to use the SGD
to minimize the loss by computing the gradient with respect to a randomly selected batch from the training set. This method is more efficient than computing the gradient with respect to the whole training set before each update is performed.
\begin{equation}
\frac{\partial p_i}{\partial a_j}=\left\{\begin{matrix} p_i(1-p_i) & if & i=j\\ -p_j p_i & if & i\neq j \end{matrix}\right.
\end{equation} \\
\begin{equation}
\frac{\partial L}{\partial o_i}=p_i-y\_oh_i
\end{equation} 
For the derivation of the cross entropy loss, \textit{y\_oh} is the one-hot encoded representation of the class labels.
\subsubsection{Softmax}
The softmax normalizes the class probabilities to one and it has a probabilistic interpretation. 
\begin{equation}
f_{j}(z) = \frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}
\end{equation}
The exponential values can very quickly explode to an infinite large number, for example \(e_{1000}\). To fix this issue, it takes a one-dimensional vector of arbitrary length (in \textit{z}) and puts it into a vector of values between zero and one that sum together to one. The cross-entropy loss that includes the softmax function, hence to minimize the cross-entropy-loss between the estimated class probabilites. \\
\subsubsection{Flatten}
In between the convolutional layer (CNN) and the fully connected layer (Dense), there is a \textit{Flatten layer}. The Flattening layer transforms a two-dimensional matrix of features into a one-dimensional vector that can be respectively streamed into the fully connected neural network classifier, which are our ten fully connected animal neurons. \\
%
% === IV. Result Analysis============================
%
\section{Result Analysis}
\subsection{Evaluation Criteria}
\subsubsection{Overfitting and Generalisation}
\subsubsection{Possible Improvements}
% ============================================
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Bibliography}
\vfill
\end{document}